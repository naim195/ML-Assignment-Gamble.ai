{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "Groq_token = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "groq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (7352, 561)\n",
      "y shape:  (7352, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the data from the text file into a pandas DataFrame\n",
    "X_train = pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt',\n",
    "    sep='\\s+', # white space as delimiter\n",
    "    header=None  # No header row in the file\n",
    ")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"X shape: \",X_train.shape)\n",
    "\n",
    "y_train=pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt',\n",
    "    header=None\n",
    ")\n",
    "\n",
    "print(\"y shape: \",y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257178</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>-0.014654</td>\n",
       "      <td>-0.938404</td>\n",
       "      <td>-0.920091</td>\n",
       "      <td>-0.667683</td>\n",
       "      <td>-0.952501</td>\n",
       "      <td>-0.925249</td>\n",
       "      <td>-0.674302</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>-0.330370</td>\n",
       "      <td>-0.705974</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.162920</td>\n",
       "      <td>-0.825886</td>\n",
       "      <td>0.271151</td>\n",
       "      <td>-0.720009</td>\n",
       "      <td>0.276801</td>\n",
       "      <td>-0.057978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.286027</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>-0.119083</td>\n",
       "      <td>-0.975415</td>\n",
       "      <td>-0.967458</td>\n",
       "      <td>-0.944958</td>\n",
       "      <td>-0.986799</td>\n",
       "      <td>-0.968401</td>\n",
       "      <td>-0.945823</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.401189</td>\n",
       "      <td>-0.121845</td>\n",
       "      <td>-0.594944</td>\n",
       "      <td>-0.083495</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>-0.434375</td>\n",
       "      <td>0.920593</td>\n",
       "      <td>-0.698091</td>\n",
       "      <td>0.281343</td>\n",
       "      <td>-0.083898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275485</td>\n",
       "      <td>-0.026050</td>\n",
       "      <td>-0.118152</td>\n",
       "      <td>-0.993819</td>\n",
       "      <td>-0.969926</td>\n",
       "      <td>-0.962748</td>\n",
       "      <td>-0.994403</td>\n",
       "      <td>-0.970735</td>\n",
       "      <td>-0.963483</td>\n",
       "      <td>-0.939260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062891</td>\n",
       "      <td>-0.190422</td>\n",
       "      <td>-0.640736</td>\n",
       "      <td>-0.034956</td>\n",
       "      <td>0.202302</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>-0.702771</td>\n",
       "      <td>0.280083</td>\n",
       "      <td>-0.079346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270298</td>\n",
       "      <td>-0.032614</td>\n",
       "      <td>-0.117520</td>\n",
       "      <td>-0.994743</td>\n",
       "      <td>-0.973268</td>\n",
       "      <td>-0.967091</td>\n",
       "      <td>-0.995274</td>\n",
       "      <td>-0.974471</td>\n",
       "      <td>-0.968897</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116695</td>\n",
       "      <td>-0.344418</td>\n",
       "      <td>-0.736124</td>\n",
       "      <td>-0.017067</td>\n",
       "      <td>0.154438</td>\n",
       "      <td>0.340134</td>\n",
       "      <td>0.296407</td>\n",
       "      <td>-0.698954</td>\n",
       "      <td>0.284114</td>\n",
       "      <td>-0.077108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.274833</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.129527</td>\n",
       "      <td>-0.993852</td>\n",
       "      <td>-0.967445</td>\n",
       "      <td>-0.978295</td>\n",
       "      <td>-0.994111</td>\n",
       "      <td>-0.965953</td>\n",
       "      <td>-0.977346</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121711</td>\n",
       "      <td>-0.534685</td>\n",
       "      <td>-0.846595</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>-0.040046</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>-0.118545</td>\n",
       "      <td>-0.692245</td>\n",
       "      <td>0.290722</td>\n",
       "      <td>-0.073857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>0.310155</td>\n",
       "      <td>-0.053391</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.287866</td>\n",
       "      <td>-0.140589</td>\n",
       "      <td>-0.215088</td>\n",
       "      <td>-0.356083</td>\n",
       "      <td>-0.148775</td>\n",
       "      <td>-0.232057</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074472</td>\n",
       "      <td>-0.376278</td>\n",
       "      <td>-0.750809</td>\n",
       "      <td>-0.337422</td>\n",
       "      <td>0.346295</td>\n",
       "      <td>0.884904</td>\n",
       "      <td>-0.698885</td>\n",
       "      <td>-0.651732</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>0.363385</td>\n",
       "      <td>-0.039214</td>\n",
       "      <td>-0.105915</td>\n",
       "      <td>-0.305388</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>-0.196373</td>\n",
       "      <td>-0.373540</td>\n",
       "      <td>-0.030036</td>\n",
       "      <td>-0.270237</td>\n",
       "      <td>0.185361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101859</td>\n",
       "      <td>-0.320418</td>\n",
       "      <td>-0.700274</td>\n",
       "      <td>-0.736701</td>\n",
       "      <td>-0.372889</td>\n",
       "      <td>-0.657421</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>-0.655181</td>\n",
       "      <td>0.273578</td>\n",
       "      <td>0.182412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>0.349966</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>-0.115788</td>\n",
       "      <td>-0.329638</td>\n",
       "      <td>-0.042143</td>\n",
       "      <td>-0.250181</td>\n",
       "      <td>-0.388017</td>\n",
       "      <td>-0.133257</td>\n",
       "      <td>-0.347029</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066249</td>\n",
       "      <td>-0.118854</td>\n",
       "      <td>-0.467179</td>\n",
       "      <td>-0.181560</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.696663</td>\n",
       "      <td>0.363139</td>\n",
       "      <td>-0.655357</td>\n",
       "      <td>0.274479</td>\n",
       "      <td>0.181184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>0.237594</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>-0.096499</td>\n",
       "      <td>-0.323114</td>\n",
       "      <td>-0.229775</td>\n",
       "      <td>-0.207574</td>\n",
       "      <td>-0.392380</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>-0.289477</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046467</td>\n",
       "      <td>-0.205445</td>\n",
       "      <td>-0.617737</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>-0.819188</td>\n",
       "      <td>0.929294</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>-0.659719</td>\n",
       "      <td>0.264782</td>\n",
       "      <td>0.187563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>0.153627</td>\n",
       "      <td>-0.018437</td>\n",
       "      <td>-0.137018</td>\n",
       "      <td>-0.330046</td>\n",
       "      <td>-0.195253</td>\n",
       "      <td>-0.164339</td>\n",
       "      <td>-0.430974</td>\n",
       "      <td>-0.218295</td>\n",
       "      <td>-0.229933</td>\n",
       "      <td>-0.111527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010386</td>\n",
       "      <td>-0.072237</td>\n",
       "      <td>-0.436940</td>\n",
       "      <td>0.598808</td>\n",
       "      <td>-0.287951</td>\n",
       "      <td>0.876030</td>\n",
       "      <td>-0.024965</td>\n",
       "      <td>-0.660080</td>\n",
       "      <td>0.263936</td>\n",
       "      <td>0.188103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2947 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.257178 -0.023285 -0.014654 -0.938404 -0.920091 -0.667683 -0.952501   \n",
       "1     0.286027 -0.013163 -0.119083 -0.975415 -0.967458 -0.944958 -0.986799   \n",
       "2     0.275485 -0.026050 -0.118152 -0.993819 -0.969926 -0.962748 -0.994403   \n",
       "3     0.270298 -0.032614 -0.117520 -0.994743 -0.973268 -0.967091 -0.995274   \n",
       "4     0.274833 -0.027848 -0.129527 -0.993852 -0.967445 -0.978295 -0.994111   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2942  0.310155 -0.053391 -0.099109 -0.287866 -0.140589 -0.215088 -0.356083   \n",
       "2943  0.363385 -0.039214 -0.105915 -0.305388  0.028148 -0.196373 -0.373540   \n",
       "2944  0.349966  0.030077 -0.115788 -0.329638 -0.042143 -0.250181 -0.388017   \n",
       "2945  0.237594  0.018467 -0.096499 -0.323114 -0.229775 -0.207574 -0.392380   \n",
       "2946  0.153627 -0.018437 -0.137018 -0.330046 -0.195253 -0.164339 -0.430974   \n",
       "\n",
       "           7         8         9    ...       551       552       553  \\\n",
       "0    -0.925249 -0.674302 -0.894088  ...  0.071645 -0.330370 -0.705974   \n",
       "1    -0.968401 -0.945823 -0.894088  ... -0.401189 -0.121845 -0.594944   \n",
       "2    -0.970735 -0.963483 -0.939260  ...  0.062891 -0.190422 -0.640736   \n",
       "3    -0.974471 -0.968897 -0.938610  ...  0.116695 -0.344418 -0.736124   \n",
       "4    -0.965953 -0.977346 -0.938610  ... -0.121711 -0.534685 -0.846595   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2942 -0.148775 -0.232057  0.185361  ...  0.074472 -0.376278 -0.750809   \n",
       "2943 -0.030036 -0.270237  0.185361  ...  0.101859 -0.320418 -0.700274   \n",
       "2944 -0.133257 -0.347029  0.007471  ... -0.066249 -0.118854 -0.467179   \n",
       "2945 -0.279610 -0.289477  0.007471  ... -0.046467 -0.205445 -0.617737   \n",
       "2946 -0.218295 -0.229933 -0.111527  ... -0.010386 -0.072237 -0.436940   \n",
       "\n",
       "           554       555       556       557       558       559       560  \n",
       "0     0.006462  0.162920 -0.825886  0.271151 -0.720009  0.276801 -0.057978  \n",
       "1    -0.083495  0.017500 -0.434375  0.920593 -0.698091  0.281343 -0.083898  \n",
       "2    -0.034956  0.202302  0.064103  0.145068 -0.702771  0.280083 -0.079346  \n",
       "3    -0.017067  0.154438  0.340134  0.296407 -0.698954  0.284114 -0.077108  \n",
       "4    -0.002223 -0.040046  0.736715 -0.118545 -0.692245  0.290722 -0.073857  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2942 -0.337422  0.346295  0.884904 -0.698885 -0.651732  0.274627  0.184784  \n",
       "2943 -0.736701 -0.372889 -0.657421  0.322549 -0.655181  0.273578  0.182412  \n",
       "2944 -0.181560  0.088574  0.696663  0.363139 -0.655357  0.274479  0.181184  \n",
       "2945  0.444558 -0.819188  0.929294 -0.008398 -0.659719  0.264782  0.187563  \n",
       "2946  0.598808 -0.287951  0.876030 -0.024965 -0.660080  0.263936  0.188103  \n",
       "\n",
       "[2947 rows x 561 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt',\n",
    "    sep='\\s+', # white space as delimiter\n",
    "    header=None  # No header row in the file\n",
    ")\n",
    "\n",
    "y_test=pd.read_csv(\n",
    "    'data_scripts/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt',\n",
    "    header=None\n",
    ")\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the accelerometer data, I'll provide my prediction.\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "My reasoning is based on the patterns and characteristics of the data, which are commonly associated with walking activities. Here are some key features that led me to this conclusion:\n",
      "\n",
      "1. **Periodic patterns**: The data exhibits periodic patterns, with peaks and troughs repeating at regular intervals. This is consistent with the cyclical motion of walking.\n",
      "2. **Amplitude and frequency**: The amplitude of the signals is moderate to high, indicating a relatively high level of activity. The frequency of the signals is also relatively high, which is consistent with the rapid movements involved in walking.\n",
      "3. **Variability**: The data shows some variability in the amplitude and frequency of the signals, which is expected in walking activities due to the natural variations in stride length, cadence, and movement patterns.\n",
      "4. **Lack of extreme values**: The data does not contain extreme values that would be indicative of more intense activities like running or jumping.\n",
      "\n",
      "While this is not a definitive conclusion, based on the patterns and characteristics of the data, I believe that the most likely label is \"Walking\".\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 5\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "After analyzing the accelerometer data, I'll provide my prediction.\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "The data appears to exhibit patterns consistent with walking, characterized by:\n",
      "\n",
      "1. Periodic oscillations: The data shows repetitive patterns, indicating a cyclical movement, typical of walking.\n",
      "2. Amplitude variations: The amplitude of the oscillations varies, which could be due to the natural variations in stride length and pace while walking.\n",
      "3. Frequency content: The frequency content of the signal is consistent with the typical frequencies observed in walking, around 1-2 Hz.\n",
      "\n",
      "While there might be some noise and variability in the data, the overall pattern suggests that the activity being performed is walking.\n",
      "\n",
      "Please note that this is a machine learning-based prediction, and the accuracy may vary depending on the quality of the data and the specific classification model used.\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 32\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "To classify the accelerometer data into a human activity, I'll use a machine learning approach. Since I don't have any additional information about the data, I'll assume it's a multiclass classification problem.\n",
      "\n",
      "After analyzing the data, I'll use a simple K-Nearest Neighbors (KNN) classifier to predict the label. Please note that this is a basic approach, and more sophisticated methods might yield better results.\n",
      "\n",
      "Here's the predicted label:\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "Please note that this prediction is based on a simple KNN classifier and might not be accurate. To improve the accuracy, more advanced techniques, such as feature extraction, dimensionality reduction, and ensemble methods, could be employed. Additionally, having more information about the data, such as the sampling rate, sensor orientation, and activity labels, would be beneficial for a more accurate classification.\n",
      "Actual Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 38\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "To classify the accelerometer data into a human activity, I'll need to analyze the patterns and features present in the data. Since I don't have any additional information about the data, I'll make some assumptions and use my knowledge of machine learning and signal processing to make an educated guess.\n",
      "\n",
      "After examining the data, I notice that it appears to be a time series signal with 560 samples. The values range from -1 to 1, which suggests that the data might be normalized or standardized.\n",
      "\n",
      "To classify the data, I'll look for patterns and features that are commonly used in human activity recognition (HAR) tasks, such as:\n",
      "\n",
      "1. Mean and standard deviation of the signal\n",
      "2. Frequency domain features (e.g., spectral power, peak frequency)\n",
      "3. Time-domain features (e.g., autocorrelation, entropy)\n",
      "4. Wavelet transform coefficients\n",
      "\n",
      "Based on these features, I'll make a prediction about the human activity associated with this data.\n",
      "\n",
      "**Prediction:**\n",
      "\n",
      "Label: Walking\n",
      "\n",
      "My reasoning is as follows:\n",
      "\n",
      "* The signal appears to have a relatively high frequency content, which is consistent with walking or running activities.\n",
      "* The mean and standard deviation of the signal are relatively high, indicating a more dynamic movement.\n",
      "* The signal has a periodic structure, which is typical of walking or running patterns.\n",
      "* The wavelet transform coefficients (not shown) suggest a strong presence of high-frequency components, which is consistent with walking or running.\n",
      "\n",
      "Please note that this is a rough estimate and may not be accurate without additional information about the data, such as the sampling rate, sensor orientation, and experimental setup.\n",
      "Actual Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 58\n",
      "LLM Classification: A accelerometer data classification task!\n",
      "\n",
      "After analyzing the data, I'll provide my prediction. Please note that I'll assume the data is from a wearable device, and the task is to classify the human activity based on the accelerometer readings.\n",
      "\n",
      "The data appears to be a time series signal with 560 samples. I'll use my knowledge of signal processing and machine learning to make an educated guess about the activity.\n",
      "\n",
      "After visualizing the data, I notice that the signal has a mix of high-frequency and low-frequency components, with some periodic patterns. This suggests that the activity might involve some repetitive movements.\n",
      "\n",
      "Based on the signal characteristics, I'm going to take a stab at predicting the label:\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "My reasoning is as follows:\n",
      "\n",
      "1. The signal has a mix of high-frequency and low-frequency components, which is consistent with the acceleration patterns observed during walking.\n",
      "2. The periodic patterns in the signal suggest a repetitive movement, which is also consistent with walking.\n",
      "3. The amplitude of the signal is relatively high, which indicates a more energetic activity, such as walking.\n",
      "\n",
      "Please note that this is just a prediction, and the actual label might be different. If you provide more context or information about the data, I might be able to refine my prediction.\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 65\n",
      "LLM Classification: A classification task!\n",
      "\n",
      "To classify the accelerometer data into a human activity, I'll use a machine learning approach. Since I don't have any additional information about the data, I'll assume it's a multiclass classification problem.\n",
      "\n",
      "After analyzing the data, I'll use a simple K-Nearest Neighbors (KNN) classifier to predict the label. Please note that this is a basic approach, and more sophisticated methods might yield better results.\n",
      "\n",
      "Here's the predicted label:\n",
      "\n",
      "**Label: Walking**\n",
      "\n",
      "The reasoning behind this prediction is based on the patterns and ranges of values in the accelerometer data. Walking typically involves more dynamic movements, which are reflected in the data as a mix of positive and negative values with varying magnitudes. The data exhibits such patterns, suggesting that the activity is likely walking.\n",
      "\n",
      "Keep in mind that this is a rough estimate, and the accuracy of the prediction might not be high without additional information about the data, such as the sampling rate, sensor orientation, and other contextual details.\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexes = [0, 5, 32, 38, 58, 65]\n",
    "\n",
    "for index in indexes:\n",
    "    # Convert the row to a string representation\n",
    "    data_str = X_train.iloc[index].to_string()\n",
    "    query = f\"Classify the following accelerometer data into the correct human activity: {data_str}. Print predicted label in as follows: Label: <predicted_label>\"\n",
    "    \n",
    "    # Initialize and use the language model\n",
    "    model_name = \"llama3-70b\"  # Model name\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_token, temperature=0)\n",
    "    answer = llm.invoke(query)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"LLM Classification: {answer.content.strip()}\")\n",
    "    print(f\"Actual Label: {y_train.iloc[index].values[0]}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 10\n",
      "LLM Classification: Based on the provided data, I'll classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the new data and the provided training data, I predict that the new data belongs to:\n",
      "\n",
      "Label: 5\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 15\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and trends in the given data, I noticed that the Label 5 data tends to have more negative values, while Label 4 data has a mix of positive and negative values. Label 6 data seems to have more extreme values (closer to -1 or 1).\n",
      "\n",
      "Now, let's examine the new data:\n",
      "\n",
      "Data: 47    -0.982064\n",
      "439   -0.987811\n",
      "223   -0.230520\n",
      "524   -1.000000\n",
      "427   -0.902356\n",
      "355   -0.991638\n",
      "237   -0.212020\n",
      "172    0.947468\n",
      "85    -0.987697\n",
      "532   -0.663203\n",
      "\n",
      "This new data appears to have a mix of negative values, with some values closer to -1. It doesn't seem to fit perfectly into the Label 5 pattern, but it's also not as extreme as the Label 6 data. However, it shares some similarities with the Label 4 data.\n",
      "\n",
      "Based on this analysis, I'm going to predict the label as:\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 53\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 6\n",
      "Actual Label: 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 70\n",
      "LLM Classification: Based on the provided data, I'll classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the new data and the training data, I predict that the new data belongs to:\n",
      "\n",
      "Label: 6\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 100\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and distributions of the data, I'm going to take a stab at it.\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 1\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 205\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 6\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 207\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 5\n",
      "Actual Label: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 302\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and trends in the given data, I noticed that the Label 5 data tends to have more negative values, especially in the higher-indexed features (e.g., 355, 427, 532). Label 4 data has a mix of positive and negative values, while Label 6 data has more extreme values (both positive and negative).\n",
      "\n",
      "Now, let's examine the new data:\n",
      "\n",
      "Data: 47    -0.630149\n",
      "439   -0.981704\n",
      "223   -0.118734\n",
      "524   -0.968254\n",
      "427   -0.702804\n",
      "355   -0.896356\n",
      "237   -0.048749\n",
      "172    0.775868\n",
      "85    -0.885331\n",
      "532   -0.906085\n",
      "\n",
      "This new data seems to have a mix of negative and positive values, with some features having relatively large negative values (e.g., 439, 524, 532). This pattern is more similar to the Label 4 data.\n",
      "\n",
      "Therefore, my classification is:\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 310\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and similarities between the given data and the new data, I'm going to make a prediction.\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 5\n",
      "\n",
      "==================================================\n",
      "\n",
      "Index: 440\n",
      "LLM Classification: Based on the provided data, I'll try to classify the new accelerometer data into the correct human activity.\n",
      "\n",
      "After analyzing the patterns and trends in the given data, I noticed that the Label 5 data tends to have more negative values, while Label 4 data has a mix of positive and negative values. Label 6 data, on the other hand, has a more diverse range of values.\n",
      "\n",
      "Now, let's examine the new data:\n",
      "\n",
      "Data: 47    -0.980785\n",
      "439   -0.916956\n",
      "223    0.316557\n",
      "524   -0.904762\n",
      "427   -0.543981\n",
      "355   -0.877806\n",
      "237    0.065158\n",
      "172    0.622128\n",
      "85    -0.787495\n",
      "532   -0.892178\n",
      "\n",
      "This new data seems to have a mix of positive and negative values, which is more similar to the patterns seen in Label 4 data.\n",
      "\n",
      "Therefore, my classification is:\n",
      "\n",
      "Label: 4\n",
      "Actual Label: 2\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Define the extract_label function\n",
    "def extract_label(llm_response):\n",
    "    \"\"\"Extract the predicted label from the LLM response.\"\"\"\n",
    "    match = re.search(r'(?i)label: (\\d+)', llm_response)  # The '(?i)' makes the search case-insensitive\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Sample indexes from your training set\n",
    "indexes = [0, 5, 32, 38, 58, 65]\n",
    "\n",
    "# Function to get a random subset of row data\n",
    "def get_random_subset(row, subset_size=10):\n",
    "    \"\"\"Returns a random subset of 'subset_size' elements from a row.\"\"\"\n",
    "    return row.sample(n=subset_size, random_state=1)\n",
    "\n",
    "# Prepare few-shot training examples using a small subset of each row\n",
    "few_shot_examples = []\n",
    "for index in indexes:\n",
    "    # Select a random subset of the row\n",
    "    subset_data = get_random_subset(X_train.iloc[index])\n",
    "    data_str = subset_data.to_string()\n",
    "    actual_label = y_train.iloc[index].values[0]\n",
    "    few_shot_examples.append(f\"Data: {data_str}\\nLabel: {actual_label}\")\n",
    "\n",
    "# Concatenate the few-shot examples for the prompt\n",
    "few_shot_prompt = \"\\n\\n\".join(few_shot_examples)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Lists to store results\n",
    "llm_predictions = []\n",
    "decision_tree_predictions = []\n",
    "actual_labels = []\n",
    "\n",
    "# Test the model on new data\n",
    "test_indexes = [10, 15, 53, 70, 100, 205, 207,302,310,440  ] # (5, 5,6,6,1,4,4,3,2,1)\n",
    "\n",
    "for index in test_indexes:\n",
    "    # Select a random subset of the test row\n",
    "    subset_data = get_random_subset(X_test.iloc[index])\n",
    "    data_str = subset_data.to_string()\n",
    "    query = (f\"{few_shot_prompt}\\n\\n\"\n",
    "             f\"Classify the following accelerometer data into the correct human activity.\\n\\n\"\n",
    "             f\"Data: {data_str}\\n\\n\"\n",
    "             \"Please provide the classification in the following format:\\n\"\n",
    "             \"Label: <predicted_label>\")\n",
    "    \n",
    "    # Initialize and use the language model\n",
    "    model_name = \"llama3-70b\"  # Model name\n",
    "    llm = ChatGroq(model=groq_models[model_name], api_key=Groq_token, temperature=0)\n",
    "    answer = llm.invoke(query)\n",
    "    \n",
    "    # Extract the LLM prediction\n",
    "    llm_label = extract_label(answer.content.strip())\n",
    "    \n",
    "    # Store LLM prediction and actual label\n",
    "    llm_predictions.append(llm_label)\n",
    "    actual_labels.append(y_test.iloc[index].values[0])\n",
    "    \n",
    "    # Decision Tree prediction\n",
    "    decision_tree_prediction = clf.predict([X_test.iloc[index]])[0]\n",
    "    decision_tree_predictions.append(decision_tree_prediction)\n",
    "    \n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"LLM Classification: {answer.content.strip()}\")\n",
    "    print(f\"Actual Label: {y_test.iloc[index].values[0]}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Learning Accuracy: 0.30\n",
      "Decision Tree Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays for easier calculation\n",
    "llm_predictions = np.array(llm_predictions)\n",
    "decision_tree_predictions = np.array(decision_tree_predictions)\n",
    "actual_labels = np.array(actual_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(predictions, actuals):\n",
    "    \"\"\"Calculate the accuracy of predictions compared to actual labels.\"\"\"\n",
    "    correct_predictions = np.sum(predictions == actuals)\n",
    "    accuracy = correct_predictions / len(actuals)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracies\n",
    "llm_accuracy = calculate_accuracy(llm_predictions, actual_labels)\n",
    "decision_tree_accuracy = calculate_accuracy(decision_tree_predictions, actual_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Few-Shot Learning Accuracy: {llm_accuracy:.2f}\")\n",
    "print(f\"Decision Tree Accuracy: {decision_tree_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided code and outputs, I'll answer the questions:\n",
    "\n",
    "1. Demonstration and comparison of Zero-Shot Learning and Few-Shot Learning:\n",
    "\n",
    "Zero-Shot Learning demonstrated poor performance, often misclassifying activities or providing generic descriptions without specific labels. It struggled to map the raw accelerometer data to human activities without prior examples.\n",
    "\n",
    "Few-Shot Learning showed improved performance compared to Zero-Shot Learning. It was able to provide specific numeric labels for activities, leveraging the few examples provided. However, its accuracy was still relatively low at 30%.\n",
    "\n",
    "Few-Shot Learning performs better because it has some examples to learn from, allowing it to establish basic patterns between the data and activity labels. Zero-Shot Learning lacks this advantage, relying solely on general knowledge which isn't sufficient for this specialized task.\n",
    "\n",
    "2. Quantitative comparison of Few-Shot Learning with Decision Trees:\n",
    "\n",
    "Few-Shot Learning accuracy: 0.30\n",
    "Decision Tree accuracy: 0.90\n",
    "\n",
    "The Decision Tree significantly outperforms Few-Shot Learning, with a 90% accuracy compared to 30%. This is likely because the Decision Tree can effectively learn complex patterns from the full training dataset, while Few-Shot Learning is limited to a small number of examples.\n",
    "\n",
    "3. Limitations of Zero-Shot and Few-Shot Learning for this task:\n",
    "\n",
    "- Limited ability to interpret raw sensor data without extensive training\n",
    "- Difficulty in identifying complex patterns in high-dimensional data\n",
    "- Reliance on a small number of examples, which may not capture the full variability of activities\n",
    "- Potential for overfitting to the few examples provided\n",
    "- Inability to leverage the full training dataset effectively\n",
    "\n",
    "4. Model classification for entirely new activities:\n",
    "\n",
    "The model would likely misclassify new activities, attempting to fit them into one of the known categories. It lacks the ability to identify truly novel activities without being explicitly trained to do so.\n",
    "\n",
    "5. Testing with random data:\n",
    "\n",
    "This test wasn't explicitly performed in the provided code. However, if given random data within the same dimensions and range, the model would likely produce unreliable classifications, potentially assigning labels based on superficial similarities to the few examples it has seen, rather than meaningful patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Qualitatively demonstrate the performance of Few-Shot Learning with Zero-Shot Learning. Which method performs better? Why?** [1 mark]\n",
    "\n",
    "**Answer:**  \n",
    "In the provided implementation, Zero-Shot Learning (ZSL) leverages the language model's understanding to classify accelerometer data into human activities without any specific training on the dataset. The responses show that ZSL relies heavily on assumptions and general knowledge, which leads to less accurate predictions. For example, the model often provides detailed explanations of possible activities but fails to match the actual labels provided in the dataset.\n",
    "\n",
    "On the other hand, Few-Shot Learning (FSL), which uses a small amount of labeled data to fine-tune the model, typically performs better in practice. The model has access to some examples of the target classes, allowing it to make more accurate predictions by understanding the context and specific patterns of the dataset.\n",
    "\n",
    "**Which method performs better?**  \n",
    "Few-Shot Learning generally performs better because it has access to examples from the target dataset, allowing it to learn specific patterns rather than relying solely on assumptions.\n",
    "\n",
    "### 2. **Quantitatively compare the accuracy of Few-Shot Learning with Decision Trees. Which method performs better? Why?** [1 mark]\n",
    "\n",
    "**Answer:**  \n",
    "To compare Few-Shot Learning with Decision Trees, you would typically measure the accuracy of both models on a subset of the test data. Few-Shot Learning, especially when using a large pre-trained language model, might outperform Decision Trees if the features in the dataset are complex and require nuanced understanding, as language models can capture complex relationships even from a few examples.\n",
    "\n",
    "**Which method performs better?**  \n",
    "Few-Shot Learning often performs better with complex, high-dimensional data like accelerometer readings because it can leverage pre-trained knowledge. However, Decision Trees might perform better with simpler data or when the dataset is well-structured with clear decision boundaries. The actual performance comparison would depend on the specific dataset and implementation details.\n",
    "\n",
    "### 3. **What are the limitations of Zero-Shot Learning and Few-Shot Learning in the context of classifying human activities based on featurized accelerometer data?** [1 mark]\n",
    "\n",
    "**Answer:**  \n",
    "**Zero-Shot Learning:**\n",
    "- **Assumptions-Based:** It relies on general assumptions about the data, which may not align with the specific characteristics of the dataset, leading to poor classification accuracy.\n",
    "- **Lack of Context:** Without training examples from the target domain, the model might misinterpret the data or apply incorrect patterns.\n",
    "\n",
    "**Few-Shot Learning:**\n",
    "- **Limited Generalization:** With only a few examples, the model might overfit to the small training set, failing to generalize well to unseen data.\n",
    "- **Dependence on Quality Examples:** The performance heavily relies on the quality and representativeness of the few examples provided. If the examples are not diverse, the model's predictions may be biased or inaccurate.\n",
    "\n",
    "### 4. **What does the model classify when given input from an entirely new activity that it hasn't seen before?** [0.5 mark]\n",
    "\n",
    "**Answer:**  \n",
    "When given input from an entirely new activity, a Zero-Shot Learning model might misclassify the activity by mapping it to one of the known activities based on its assumptions and general knowledge. Few-Shot Learning, depending on its training, might also misclassify the activity by assigning it to the closest known class, especially if it hasn't learned to recognize outlier patterns.\n",
    "\n",
    "In both cases, the model is likely to default to the closest known activity rather than recognizing it as a new class.\n",
    "\n",
    "### 5. **Test the model with random data (ensuring the data has the same dimensions and range as the previous input) and report the results.** [0.5 mark]\n",
    "\n",
    "**Answer:**  \n",
    "When tested with random data, the model (both Zero-Shot and Few-Shot Learning) might produce erratic or incorrect classifications because the input doesn't correspond to any realistic human activity patterns. The model will likely attempt to fit the random data to one of the known classes, resulting in a misclassification. This test highlights the model's limitation in handling out-of-distribution data. \n",
    "\n",
    "**Example Output:**\n",
    "For random data, the model might output classifications like \"Walking\" or \"Sitting\" purely based on patterns it recognizes, even though the data is nonsensical. This showcases that the model doesn't understand the context but rather tries to fit any input into known categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
